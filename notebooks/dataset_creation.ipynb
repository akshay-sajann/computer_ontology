{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uAIvuf06Hj7"
   },
   "source": [
    "Making the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3070,
     "status": "ok",
     "timestamp": 1713439688138,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "ii56e_kdt9Ka"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/miniconda3/envs/ontology/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pyrfume\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import rdkit.Chem as Chem\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from computer_ontology.custom_funcs import get_dataset\n",
    "from computer_ontology.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1713439688459,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "RrW1yBibue_b"
   },
   "outputs": [],
   "source": [
    "def leffingwell_reverse_one_hot(row):\n",
    "    \"\"\"\n",
    "    Takes a row of the Leffingwell dataset\n",
    "    and reverses one-hot-encoding.\n",
    "\n",
    "    :param row: A given row of the Leffingwell dataset.\n",
    "    :type row: pandas Dataframe row\n",
    "    :return: A list of classes/labels for each row.\n",
    "    :rtype: List\n",
    "    \"\"\"\n",
    "    labels = [col for col in leffingwell.columns if row[col] == 1]\n",
    "    return ';'.join(labels)\n",
    "\n",
    "def dravnieks_top_n_columns(row, n):\n",
    "    \"\"\"\n",
    "    This is for the Dravnieks dataset\n",
    "    since its format is unique. It\n",
    "    selects the labels with the highest\n",
    "    values and returns them in the standard\n",
    "    format used for the other datasets.\n",
    "\n",
    "    :param row: A given row of the Dravnieks dataset.\n",
    "    :type row: pandas Dataframe row\n",
    "    :param n: The number of labels to consider.\n",
    "    :type n: int\n",
    "    :return: top n labels.\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    sorted_columns = row.sort_values(ascending=False).index\n",
    "    top_n = sorted_columns[:n]\n",
    "    return ';'.join(top_n)\n",
    "\n",
    "def get_unique(df):\n",
    "    \"\"\"\n",
    "    This function takes in an odor dataset as a\n",
    "    dataframe and returns a dataframe containing\n",
    "    all the unique labels of the input dataframe.\n",
    "\n",
    "    :requirements: labels should be called 'Descriptors'.\n",
    "\n",
    "    :param df: A multilabel dataframe with labels separated by ';'\n",
    "    :type df: pandas Dataframe\n",
    "    :return: A dataframe containing the unique descriptors.\n",
    "    :rtype: pandas Dataframe\n",
    "    \"\"\"\n",
    "    all_descriptors = []\n",
    "\n",
    "    for des in df['Descriptors']:\n",
    "        all_descriptors.extend(des.split(';'))\n",
    "\n",
    "    unique_descriptors = list(set(all_descriptors))\n",
    "    unique_descriptors.sort()\n",
    "\n",
    "    df = pd.DataFrame(unique_descriptors)\n",
    "    return df\n",
    "\n",
    "def get_dataset(name):\n",
    "    \"\"\"\n",
    "    This function takes in a string which is the\n",
    "    name of the dataset and returns the fetched\n",
    "    dataset.\n",
    "\n",
    "    :param name: Name of the dataset according to Pyrfume\n",
    "    :type name: string\n",
    "    :return: The dataset called\n",
    "    :rtype: Dataframe\n",
    "    \"\"\"\n",
    "    # Load molecular and stimulus data\n",
    "    mols =  pyrfume.load_data(f'{name}/molecules.csv')[\"IsomericSMILES\"]\n",
    "    stim =  pyrfume.load_data(f'{name}/stimuli.csv')\n",
    "\n",
    "    # Deal with exceptions for behavior data\n",
    "    try:\n",
    "      behav =  pyrfume.load_data(f'{name}/behavior.csv')\n",
    "    except:\n",
    "      try:\n",
    "        behav =  pyrfume.load_data(f'{name}/behavior_1_sparse.csv')\n",
    "      except:\n",
    "        behav =  pyrfume.load_data(f'{name}/behavior_1.csv')\n",
    "\n",
    "    if name == 'ifra_2019':\n",
    "      behav['Descriptor 1'] = behav[['Descriptor 1', 'Descriptor 2', 'Descriptor 3']].astype(str).apply(';'.join, axis=1)\n",
    "\n",
    "      behav = behav['Descriptor 1']\n",
    "\n",
    "    labels = pd.merge(stim, behav, on='Stimulus')\n",
    "\n",
    "    # Deal with exceptions during Merging\n",
    "    try:\n",
    "      df = pd.merge(mols, labels, on='CID')\n",
    "    except:\n",
    "      labels.rename(columns={'new_CID': 'CID'}, inplace=True)\n",
    "      df = pd.merge(mols, labels, on='CID')\n",
    "\n",
    "    return df\n",
    "\n",
    "def check_and_replace(description):\n",
    "    \"\"\"\n",
    "    Iterates through a given \";\" separated strings\n",
    "    and replaces them with the mapping assigned by\n",
    "    any list labelled \"mapping\".\n",
    "\n",
    "    :param description: Text separated by ';'\n",
    "    :type name: string\n",
    "    :return: Text replaced according to the mapping\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    descriptors = description.split(';')\n",
    "    new_descriptors = []\n",
    "\n",
    "    for descriptor in descriptors:\n",
    "        for row in replace:\n",
    "            if descriptor == row[0]:\n",
    "                new_descriptors.append(row[1])\n",
    "\n",
    "    return ';'.join(new_descriptors)\n",
    "\n",
    "def make_unique(labels):\n",
    "   \"\"\"\n",
    "   Takes a text separated by \";\" and makes them\n",
    "   unique.\n",
    "\n",
    "   :param description: Text separated by ';'.\n",
    "   :type name: string\n",
    "   :return: words within text made unique.\n",
    "   :rtype: string\n",
    "    \"\"\"\n",
    "   return ';'.join(list(set(labels.split(';'))))\n",
    "\n",
    "def count_words(label_str):\n",
    "    \"\"\"\n",
    "    Takes a \";\" separated strings and counts them\n",
    "\n",
    "   :param description: Text separated by ';'.\n",
    "   :type name: string\n",
    "   :return: number of words separated by \";\"\n",
    "   :rtype: int\n",
    "    \"\"\"\n",
    "    return len(label_str.split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 14199,
     "status": "ok",
     "timestamp": 1713439702655,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "98RWnN07LHsM"
   },
   "outputs": [],
   "source": [
    "# Arctander\n",
    "arctander = get_dataset('arctander_1960')\n",
    "arctander.drop(['ChemicalName', 'CAS'], axis=1, inplace=True)\n",
    "arctander.rename(columns={'Labels': 'Descriptors'}, inplace=True)\n",
    "arctander = arctander.dropna()\n",
    "\n",
    "# AromaDB\n",
    "aromadb = get_dataset('aromadb')\n",
    "aromadb.drop(['Raw Descriptors', 'Modifiers'], axis=1, inplace=True)\n",
    "aromadb.rename(columns={'Filtered Descriptors': 'Descriptors'}, inplace=True)\n",
    "\n",
    "# Uncomment to include dravnieks\n",
    "'''\n",
    "# Dravnieks\n",
    "dravnieks = get_dataset('dravnieks_1985')\n",
    "dravnieks = dravnieks.drop([\"Name\", \"Conc\", \"CAS\"], axis=1)\n",
    "dravnieks['Descriptors'] = dravnieks.drop([\"CID\", \"IsomericSMILES\"], axis=1).apply(lambda row: dravnieks_top_n_columns(row, 3), axis=1)\n",
    "dravnieks = dravnieks[['CID', 'IsomericSMILES', 'Descriptors']]\n",
    "'''\n",
    "\n",
    "# FlavorDB\n",
    "flavordb = get_dataset('flavordb')\n",
    "flavordb = flavordb[['CID', 'Odor Percepts', 'IsomericSMILES']]\n",
    "flavordb.rename(columns={'Odor Percepts': 'Descriptors'}, inplace=True)\n",
    "flavordb.rename(columns={'IsomericSMILES': 'IsomericSMILES'}, inplace=True)\n",
    "flavordb = flavordb.dropna()\n",
    "\n",
    "# Flavornet\n",
    "flavornet = get_dataset('flavornet')\n",
    "flavornet.rename(columns={'Descriptors':'Descriptors'}, inplace=True)\n",
    "flavornet.rename(columns={'IsomericSMILES': 'IsomericSMILES'}, inplace=True)\n",
    "\n",
    "# Goodscents\n",
    "goodscents = get_dataset('goodscents')\n",
    "goodscents.drop(['TGSC ID', 'Concentration %','Solvent'], axis=1, inplace=True)\n",
    "goodscents = goodscents.dropna()\n",
    "\n",
    "# IFRA\n",
    "ifra = get_dataset('ifra_2019')\n",
    "ifra.rename(columns={'Descriptor 1': 'Descriptors'}, inplace=True)\n",
    "\n",
    "# Leffingwell\n",
    "leffingwell = get_dataset('leffingwell')\n",
    "\n",
    "#This filtering was done as CIDs below 0 were not well documented.\n",
    "leffingwell = leffingwell[leffingwell['CID']>0]\n",
    "\n",
    "# Apply the function to create the 'labels' column\n",
    "leffingwell['Descriptors'] = leffingwell.apply(leffingwell_reverse_one_hot, axis=1)\n",
    "\n",
    "leffingwell.rename(columns={'IsomericSMILES_x':'IsomericSMILES'}, inplace=True)\n",
    "leffingwell = leffingwell[['CID', 'IsomericSMILES', 'Descriptors']]\n",
    "leffingwell = leffingwell.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tz8mA82RONGX"
   },
   "source": [
    "Merging the Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1713439703338,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "1iT-KFSfOEaO",
    "outputId": "3e3c31e4-42df-4a81-a588-25abaadf31ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7531, 3)\n"
     ]
    }
   ],
   "source": [
    "# Concatentating all datasets\n",
    "# Uncomment to include Dravnieks\n",
    "#all_compounds = pd.concat([arctander, aromadb, dravnieks, flavordb, flavornet, goodscents, ifra, leffingwell], axis=0, ignore_index=True)\n",
    "\n",
    "all_compounds = pd.concat([arctander, aromadb, flavordb, flavornet, goodscents, ifra, leffingwell], axis=0, ignore_index=True)\n",
    "\n",
    "# Here all rows are merged on CID and labels are joined together.\n",
    "all_compounds = all_compounds.astype({'CID': 'int64'}) #Setting the CID column to dtype int64 because it works better with other code.\n",
    "\n",
    "all_compounds.reset_index(drop=True, inplace=True)\n",
    "agg_functions = {'CID': 'first', 'IsomericSMILES': 'first', 'Descriptors': lambda x: ';'.join(x)} #Takes the first CID and SMILES and joins all the descriptors together.\n",
    "all_compounds = all_compounds.groupby('CID').aggregate(agg_functions)\n",
    "print(all_compounds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR0JSGuOOdmg"
   },
   "source": [
    "Removing SMILES with dots in them as they are mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1713439703339,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "nH-Jw5svObM5",
    "outputId": "d5abcde2-740a-45f3-a7dd-71a92fc8fb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7257, 3)\n"
     ]
    }
   ],
   "source": [
    "#Get SMILES with '.' in them\n",
    "all_compounds['HasDot'] = all_compounds['IsomericSMILES'].apply(lambda x: '.' in x)\n",
    "\n",
    "#Drop all rows with '.' in SMILES\n",
    "all_compounds = all_compounds[all_compounds['HasDot'] == False]\n",
    "all_compounds.drop(['HasDot'], axis=1, inplace=True)\n",
    "print(all_compounds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STKWjaf6O7il"
   },
   "source": [
    "Removing Capitalization and Duplicates in the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1713439703538,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "8fezvKi0Ox_2"
   },
   "outputs": [],
   "source": [
    "all_compounds['Descriptors'] = all_compounds['Descriptors'].str.lower()\n",
    "all_compounds['Descriptors'] = all_compounds['Descriptors'].str.split(';').apply(set).apply(list).apply(lambda x: ';'.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hphBvStMPKjV"
   },
   "source": [
    "Canonicalization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9106,
     "status": "ok",
     "timestamp": 1713439712635,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "BFAkrGymPFb2",
    "outputId": "6210ba8d-f474-4e0d-aac7-93dbd0396d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7248, 3)\n"
     ]
    }
   ],
   "source": [
    "# labels_df is changed to a list called \"replace\" to make it go\n",
    "# through the check_and_replace function.\n",
    "\n",
    "replace = pd.read_excel(canon_path)\n",
    "replace = replace.fillna('')\n",
    "replace = replace[::-1]\n",
    "replace = replace.values.tolist()\n",
    "\n",
    "#Normalizing the labels in the combined dataset and removing hedonic descriptors\n",
    "all_compounds['Descriptors'] = all_compounds['Descriptors'].apply(check_and_replace)\n",
    "all_compounds['Descriptors'] = all_compounds['Descriptors'].apply(make_unique)\n",
    "all_compounds['Descriptors'] = all_compounds['Descriptors'].dropna()\n",
    "all_compounds = all_compounds[all_compounds['Descriptors'] != '']\n",
    "print(all_compounds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiYdBnh2RHs1"
   },
   "source": [
    "Remove odor descriptors that occur less than 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1713439712636,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "3gcNAfPdRF05",
    "outputId": "04022ebe-0f0a-45e6-fcb0-32d3864ee533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7209, 3)\n"
     ]
    }
   ],
   "source": [
    "#Checks descriptor frequency\n",
    "labels_count = all_compounds['Descriptors'].str.split(';', expand=True).stack().value_counts()\n",
    "\n",
    "#Remove labels that occur less than 30 times\n",
    "labels_to_remove = labels_count[labels_count < 30]\n",
    "all_compounds['Descriptors'] = all_compounds['Descriptors'].apply(lambda x: ';'.join([item for item in x.split(';') if item not in labels_to_remove.index]))\n",
    "all_compounds = all_compounds[all_compounds['Descriptors'] != '']\n",
    "print(all_compounds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove odor descriptors not in Expert Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'cool', 'nutty', 'no aroma', 'mild', 'orange blossom', 'soapy', 'natural', 'oily', 'clean', 'pleasant', 'aromatic', 'dried', 'sweet', 'radish', 'ambre', 'leather', 'bland', 'brown', 'powdery', 'warm', 'cortex', 'odorless', 'fresh', 'alliaceous']\n"
     ]
    }
   ],
   "source": [
    "expert_tax = pd.read_excel(expert_tax_path, index_col=0)\n",
    "labels_count = all_compounds['Descriptors'].str.split(';', expand=True).stack().value_counts()\n",
    "\n",
    "lbls_not_in_exp = set(labels_count.index).difference(expert_tax['Original Descriptors'])\n",
    "\n",
    "lbls_not_in_exp = list(lbls_not_in_exp)\n",
    "\n",
    "print((lbls_not_in_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6890, 3)\n"
     ]
    }
   ],
   "source": [
    "all_compounds['Descriptors'] = all_compounds['Descriptors'].apply(lambda x: ';'.join([item for item in x.split(';') if item not in lbls_not_in_exp]))\n",
    "all_compounds = all_compounds[all_compounds['Descriptors'] != '']\n",
    "print(all_compounds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVa8Iq7GRZFJ"
   },
   "source": [
    "Remove compounds that are heavier than 296 g/mol as it is the heaviest odorant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1713439713228,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "DPWHFn_fRYfN",
    "outputId": "7d6e18d1-b257-4b1d-fc1c-038a23e5c2de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6735, 3)\n"
     ]
    }
   ],
   "source": [
    "#Get molecular weights of compounds.\n",
    "all_compounds['MolecularWeight'] = all_compounds['IsomericSMILES'].apply(lambda x: rdMolDescriptors.CalcExactMolWt(Chem.MolFromSmiles(x)))\n",
    "\n",
    "#Remove compounds with molecular weights above 296.\n",
    "all_compounds = all_compounds[all_compounds['MolecularWeight'] <= 296]\n",
    "all_compounds = all_compounds.drop(['MolecularWeight'], axis=1)\n",
    "print(all_compounds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1713439714061,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "FXarkxlWRi3L",
    "outputId": "6cc673eb-6574-43fe-aed8-e703e359d20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6711, 3)\n"
     ]
    }
   ],
   "source": [
    "#Remove compounds containing other elements besides C, N, O, S, P.\n",
    "all_compounds['HasOtherElements'] = all_compounds['IsomericSMILES'].apply(lambda x: Chem.MolFromSmiles(x).HasSubstructMatch(Chem.MolFromSmarts('[!#6;!#7;!#8;!#16;!#15]')))\n",
    "\n",
    "all_compounds = all_compounds[all_compounds['HasOtherElements'] == False]\n",
    "all_compounds.drop(['HasOtherElements'], axis=1, inplace=True)\n",
    "print(all_compounds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stwrRjkdVezY"
   },
   "source": [
    "One hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1713439715178,
     "user": {
      "displayName": "A Sajan",
      "userId": "06499805654190845226"
     },
     "user_tz": -120
    },
    "id": "mHuImIHPTZa3"
   },
   "outputs": [],
   "source": [
    "combined_dataset = all_compounds.copy()\n",
    "\n",
    "#Turn descriptors column into lists\n",
    "combined_dataset['Descriptors'] = combined_dataset['Descriptors'].apply(lambda x: x.split(';'))\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "mlb.fit(combined_dataset['Descriptors'])\n",
    "\n",
    "data_bin = combined_dataset.join(pd.DataFrame.sparse.from_spmatrix(mlb.transform(combined_dataset['Descriptors']), index=combined_dataset.index, columns=mlb.classes_))\n",
    "# uncomment to download\n",
    "data_bin.to_csv(alldesc_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the dataset imposed with Expert derived Taxonomy (ET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_excel(expert_tax_path, index_col=0)\n",
    "# Labels that did not cluster with any other labels were labelled as 'NaN'\n",
    "labels_to_remove = labels_df[labels_df['Umbrella Terms'].isna()]['Original Descriptors']\n",
    "labels_df = labels_df.dropna()\n",
    "replace = labels_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6711, 3)\n"
     ]
    }
   ],
   "source": [
    "# Removing NaN values\n",
    "umbrella_dataset = all_compounds.copy()\n",
    "umbrella_dataset['Descriptors'] = all_compounds['Descriptors'].apply(lambda x: ';'.join([item for item in x.split(';') if item not in labels_to_remove.index]))\n",
    "umbrella_dataset = umbrella_dataset[umbrella_dataset['Descriptors'] != '']\n",
    "print(umbrella_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6711, 3)\n"
     ]
    }
   ],
   "source": [
    "# Replacing the odor descriptors with the umbrella terms\n",
    "umbrella_dataset['Descriptors'] = umbrella_dataset['Descriptors'].apply(check_and_replace)\n",
    "umbrella_dataset['Descriptors'] = umbrella_dataset['Descriptors'].apply(make_unique)\n",
    "umbrella_dataset['Descriptors'] = umbrella_dataset['Descriptors'].dropna()\n",
    "umbrella_dataset = umbrella_dataset[umbrella_dataset['Descriptors'] != '']\n",
    "print(umbrella_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn descriptors column into lists\n",
    "umbrella_dataset['Descriptors'] = umbrella_dataset['Descriptors'].apply(lambda x: x.split(';'))\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "mlb.fit(umbrella_dataset['Descriptors'])\n",
    "\n",
    "data_bin = umbrella_dataset.join(pd.DataFrame.sparse.from_spmatrix(mlb.transform(umbrella_dataset['Descriptors']), index=umbrella_dataset.index, columns=mlb.classes_))\n",
    "data_bin.to_csv(expert_dataset_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOjlcqE2JjLNgvdMv+hD1oZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ontology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
